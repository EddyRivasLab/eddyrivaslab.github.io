<!DOCTYPE html>
<html lang="en"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="A cast of tens">

    <title>Eddy and Rivas Lab Cluster Resources and How to Access Them | Eddy and Rivas Labs Resource Page</title>

    <link rel="canonical" href="https://eddyrivaslab.github.io/pages/cluster-computing-in-the-eddy-and-rivas-labs.html">

    <link href="/theme/css/tufte.css" rel="stylesheet">
    <link href="/theme/css/latex.css" rel="stylesheet">


    <!-- Haddock syntax highlighting -->
    <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">
        div.sourceCode { overflow-x: auto; }
        table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
          margin: 0; padding: 0; vertical-align: baseline; border: none; }
        table.sourceCode { width: 100%; line-height: 100%; }
        td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
        td.sourceCode { padding-left: 5px; }
        code > span.kw { color: #0000ff; } /* Keyword */
        code > span.ch { color: #008080; } /* Char */
        code > span.st { color: #008080; } /* String */
        code > span.co { color: #008000; } /* Comment */
        code > span.ot { color: #ff4000; } /* Other */
        code > span.al { color: #ff0000; } /* Alert */
        code > span.er { color: #ff0000; font-weight: bold; } /* Error */
        code > span.wa { color: #008000; font-weight: bold; } /* Warning */
        code > span.cn { } /* Constant */
        code > span.sc { color: #008080; } /* SpecialChar */
        code > span.vs { color: #008080; } /* VerbatimString */
        code > span.ss { color: #008080; } /* SpecialString */
        code > span.im { } /* Import */
        code > span.va { } /* Variable */
        code > span.cf { color: #0000ff; } /* ControlFlow */
        code > span.op { } /* Operator */
        code > span.bu { } /* BuiltIn */
        code > span.ex { } /* Extension */
        code > span.pp { color: #ff4000; } /* Preprocessor */
        code > span.do { color: #008000; } /* Documentation */
        code > span.an { color: #008000; } /* Annotation */
        code > span.cv { color: #008000; } /* CommentVar */
        code > span.at { } /* Attribute */
        code > span.in { color: #008000; } /* Information */
    </style>
    <style type="text/css">
        pre:not([class]) {
            background-color: white;
        }
    </style>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

<!--     <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/theme/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/theme/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/theme/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/theme/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/theme/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/theme/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="/theme/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/theme/favicon-16x16.png" sizes="16x16" />
    <meta name="application-name" content="Scorecard Diplomacy"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="/theme/mstile-144x144.png" /> -->

    <script src="//use.typekit.net/vpe6zfl.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>

    </head>
    <body class="">
    <div class="wrap">
        <header>
            <nav class="group">
                <h1 class="websitetitle"><a href="https://eddyrivaslab.github.io/">Eddy and Rivas Labs Resource Page</a></h1>
            </nav>
            <nav class="bottom-group">
            </nav>
        </header>
        <article>
        <h1>Eddy and Rivas Lab Cluster Resources and How to Access Them</h1>

        <p>Our high performance cluster computing is managed by
<a href="https://www.rc.fas.harvard.edu/">Harvard Research Computing</a> (RC).</p>
<h2>Overview</h2>
<p>Your RC <em>home directory</em> is something like <code>/n/home01/&lt;username&gt;</code>.
When you log in, that's where you'll land. You have 100GB of space
there. </p>
<p>Our <em>lab storage</em> is <code>/n/eddy_lab/</code>. We have 400TB of what RC calls
Tier 1 storage, which is fast but expensive. </p>
<p>Both your home directory and our lab storage are backed up nightly to
what RC calls <em>snapshots</em>, and periodically to what RC calls <em>disaster
recovery</em> (DR) backups.</p>
<p>It's convenient to be able to browse and edit your files on the
cluster directly from your laptop or desktop without logging into the
cluster. If you get on the RC VPN, you can remote mount your home
directory and/or the <code>/n/eddy_lab</code> lab filesystem on your local
machine using <code>samba</code>. (Warning: a samba mount is slow, and may
sometimes be flaky; don't rely on it except for lightweight tasks.)
Instructions are below.</p>
<p>RC also provides <em>shared scratch storage</em>, which is very fast but not backed up.  Files on the scratch storage that are older than 90 days are automatically deleted, and RC strongly frowns on playing tricks to make files look younger than they are.  Because RC occasionally moves the scratch storage to different devices, the easiest way to access it is through the &dollar;SCRATCH variable, which is defined on all RC machines.  Our lab has an eddy_lab directory on the scratch space with a 50TB quota, which contains a Users directory, so '&amp;dollarSCRATCH/eddy_lab/Users/yourusername' will point to your directory on the scratch space <span class="marginnote">The Users directory was pre-populated with space for a set of usernames at some point in the past.  If your username wasn't included, you'll have to email RC to get a directory created for you.</span>.  </p>
<p>The scratch space is intended for temporary data, so is a great place to put input or output files from jobs, particularly if you intend to post-process your outputs to extract a smaller amount of data from them.</p>
<p>You can read
<a href="https://docs.rc.fas.harvard.edu/kb/cluster-storage/">more documentation on how RC storage works</a>.</p>
<p>All of our lab's computing equipment is contained in the <code>eddy</code>
partition, which currently comprises 33 CPU-only nodes and 6 GPU nodes
(which also have CPUs), totalling 3264 CPU cores and 32 GPUs (12
H100's, 4 A100's, and 16 old A40's).  Most of our machines have 8GB of
RAM per CPU core. The machines are physically located in Harvard's
Holyoke data center.</p>
<p>We can also use Harvard-wide shared partitions on the RC cluster. <code>-p
shared</code> is 14,880 cores (in 310 nodes), for example (as of Feb 2025),
and there are several other partitions we can use. For more
information on these, see
<a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/#Slurm_partitions">RC's documentation on available partitions</a>.  </p>
<p>In more detail, our current cluster computing nodes are:</p>
<table>
<thead>
<tr>
<th>nodes</th>
<th>core/node</th>
<th>CPU_type</th>
<th>RAM</th>
<th>RAM/core</th>
<th>GPU/node</th>
<th>GPU_type</th>
<th>tot_core</th>
<th>tot_GPU</th>
<th>date</th>
<th>source</th>
<th>holy* node names</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>48</td>
<td>Xeon Platinum 8268 2.9GHz</td>
<td>192G</td>
<td>4G</td>
<td>-</td>
<td>-</td>
<td>240</td>
<td>-</td>
<td>Apr 2024</td>
<td>FAS</td>
<td>7c104[11-12], 7c105[01-03]</td>
</tr>
<tr>
<td>24</td>
<td>64</td>
<td>Xeon Platinum 8358 2.6GHz</td>
<td>512G</td>
<td>8G</td>
<td>-</td>
<td>-</td>
<td>1536</td>
<td>-</td>
<td>Jan 2023</td>
<td>HHMI</td>
<td>8a255[03-06] 8a256[03-06] 8a271[11,12] 8a272[07-12] 8a272[07-12] 8a273[07-12] 8a274[07,08]</td>
</tr>
<tr>
<td>4</td>
<td>192</td>
<td>AMD EPYC 9654 2.4GHz</td>
<td>1536G</td>
<td>8G</td>
<td>-</td>
<td>-</td>
<td>768</td>
<td>-</td>
<td>May 2024</td>
<td>HHMI</td>
<td>8a[28509-28512]</td>
</tr>
<tr>
<td>2</td>
<td>48</td>
<td>Xeon Platinum 8268 2.9GHz</td>
<td>768G</td>
<td>16G</td>
<td>8</td>
<td>NVIDIA A40 48G</td>
<td>96</td>
<td>16</td>
<td>Jan 2023</td>
<td>HHMI</td>
<td>gpu2c[0923,1121]</td>
</tr>
<tr>
<td>1</td>
<td>48</td>
<td>AMD EPYC 74F3 3.2GHz</td>
<td>1024G</td>
<td>21G</td>
<td>4</td>
<td>NVIDIA A100 HGX 80G</td>
<td>48</td>
<td>4</td>
<td>Jan 2023</td>
<td>HHMI</td>
<td>gpu7c0920</td>
</tr>
<tr>
<td>3</td>
<td>192</td>
<td>AMD EPYC 9654 2.4GHz</td>
<td>1536G</td>
<td>8G</td>
<td>4</td>
<td>NVIDIA H100 SXM5 80G</td>
<td>576</td>
<td>12</td>
<td>May 2024</td>
<td>HHMI</td>
<td>gpu8a[26504-26506]</td>
</tr>
</tbody>
</table>
<h2>Accessing the cluster</h2>
<h3>logging on, first time</h3>
<ul>
<li>Get a <a href="https://rc.fas.harvard.edu/resources/faq/how-do-i-get-a-research-computing-account/">Research Computing (RC) account</a>.</li>
<li>Read about how to <a href="https://docs.rc.fas.harvard.edu/kb/access-and-login/">access the RC cluster</a></li>
<li>
<p>Install <a href="https://docs.rc.fas.harvard.edu/kb/openauth/">OpenAuth two-factor authentication</a></p>
</li>
<li>
<p>Behold the glory of
  <a href="https://docs.rc.fas.harvard.edu/">RC's extensive documentation</a>,
  where most questions you have about RC are answered.</p>
</li>
<li>
<p>If you chose to install their little Java OpenAuth application on your
  machine to generate your OpenAuth codes (instead of using Duo Mobile
  or Google Authenticator on your smart phone), it's convenient to
  make an alias for launching it. In my <code>.bashrc</code>, I have <code>alias
  ody-auth='~/sw/seddy-openauth/seddy-openauth.sh &amp;'</code>, so I can launch
  my authenticator on the commandline with <code>ody-auth</code>.</p>
</li>
</ul>
<p>You should be able to ssh into the cluster now. With your username in
place of mine (<code>seddy</code>), do:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="o">%</span><span class="w"> </span><span class="n">ssh</span><span class="w"> </span><span class="n">seddy</span><span class="err">@</span><span class="n">login</span><span class="o">.</span><span class="n">rc</span><span class="o">.</span><span class="n">fas</span><span class="o">.</span><span class="n">harvard</span><span class="o">.</span><span class="n">edu</span>
</code></pre></div>

<p>It'll ask for your RC password and an OpenAuth two-factor
authentication key.</p>
<h3>configuring an ssh host alias</h3>
<p>Once you're using the cluster a lot, you can save yourself some typing
by setting up a host alias. Mine is called <strong>ody</strong>, because the RC
cluster used to be called Odyssey.  Add something like this to your
<code>.ssh/config</code> file, using your preferred host alias in place of <code>ody</code>
and your username in place of <code>seddy</code>:</p>
<div class="highlight"><pre><span></span><code>Host<span class="w">                  </span>ody
<span class="w">  </span>HostName<span class="w">            </span>login.rc.fas.harvard.edu
<span class="w">  </span>User<span class="w">                </span>seddy
<span class="w">  </span>Compression<span class="w">         </span>yes
<span class="w">  </span>ForwardX11Trusted<span class="w">   </span>yes
<span class="w">  </span>ServerAliveInterval<span class="w"> </span><span class="m">30</span>
</code></pre></div>

<p>Now you can access RC just by:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="c">% ssh ody</span>
</code></pre></div>

<p>You still have to authenticate by password and OpenAuth code, though.</p>
<h3>configuring single sign-on scp access</h3>
<p>Even better, but a little more complicated: you can make it so you
only have to authenticate once, and every ssh or scp after that is
passwordless. To do this, I use
<a href="https://docs.rc.fas.harvard.edu/kb/using-ssh-controlmaster-for-single-sign-on/">SSH ControlMaster for single sign-on</a>,
to open a single <code>ssh</code> connection that you authenticate once, and all
subsequent <code>ssh</code>-based traffic to RC goes via that connection.</p>
<p>RC's
<a href="https://docs.rc.fas.harvard.edu/kb/using-ssh-controlmaster-for-single-sign-on/">instructions are here</a>
but briefly:</p>
<ul>
<li>Replace the above hostname alias in <code>.ssh/config</code> file with
  something like this:</li>
</ul>
<div class="highlight"><pre><span></span><code>Host<span class="w">              </span>ody
<span class="w">   </span>User<span class="w">           </span>seddy
<span class="w">   </span>HostName<span class="w">       </span>login.rc.fas.harvard.edu
<span class="w">   </span>ControlMaster<span class="w">  </span>auto
<span class="w">   </span>ControlPath<span class="w">    </span>~/.ssh/%r@%h:%p
<span class="w">   </span>ControlPersist<span class="w"> </span>yes
</code></pre></div>

<ul>
<li>Add some aliases to your <code>.bashrc</code> file:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="w">   </span><span class="nb">alias</span><span class="w"> </span>ody-start<span class="o">=</span><span class="s1">&#39;ssh -Y -o ServerAliveInterval=30 -fN ody&#39;</span><span class="w">   </span>
<span class="w">   </span><span class="nb">alias</span><span class="w"> </span>ody-stop<span class="o">=</span><span class="s1">&#39;ssh -O stop ody&#39;</span>
<span class="w">   </span><span class="nb">alias</span><span class="w"> </span>ody-kill<span class="o">=</span><span class="s1">&#39;ssh -O exit ody&#39;</span>
</code></pre></div>

<p>Now you can launch a session with:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="c">% ody-start</span>
</code></pre></div>

<p>It'll ask you to authenticate. After you do this, all your ssh-based
commands (in any terminal window) will work without further
authentication. To stop the connection, do</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="c">% ody-stop</span>
</code></pre></div>

<p>If you forget to stop it, no big deal, the connection will eventually
time out by itself.</p>
<hr>
<h2>Accessing our storage</h2>
<h3>set up VPN access</h3>
<p>You don't need to be on the RC VPN to log in to the cluster, but you do
need to be on the VPN if you want to mount any of our RC storage on
your local machine.</p>
<ul>
<li>Set up <a href="https://rc.fas.harvard.edu/resources/vpn-setup/">VPN access to Odyssey</a>.</li>
</ul>
<h3>mounting our lab filesystem on your machine</h3>
<p>You need to be on the RC VPN to remote mount our filesystem.</p>
<p>From the Mac OS/X Finder, choose Go-&gt;Connect To Server, and give it:</p>
<div class="highlight"><pre><span></span><code><span class="w">   </span>smb://eddyfs.rc.fas.harvard.edu/eddy_lab
</code></pre></div>

<p>It will mount at <code>/Volumes/eddy_lab</code> on your local machine, and it
will show up in Locations in the Finder. </p>
<p>If your username on your local machine is different from your username
on the cluster, make that URL <code>smb://&lt;username&gt;:eddyfs.rc.fas.harvard.edu/eddy_lab</code>.</p>
<p>To use the OS/X command line instead of the Finder GUI:</p>
<div class="highlight"><pre><span></span><code><span class="w">   </span><span class="c1"># to mount:</span>
<span class="w">   </span>%<span class="w"> </span>osascript<span class="w"> </span>-e<span class="w"> </span><span class="err">&#39;</span>mount<span class="w"> </span>volume<span class="w"> </span><span class="s2">&quot;smb://eddyfs.rc.fas.harvard.edu/eddy_lab`</span>
<span class="s2">   # to unmount:</span>
<span class="s2">   </span>$<span class="s2"> umount /Volumes/eddy_lab</span>
</code></pre></div>

<p>You can also samba-mount your cluster home directory [<a href="https://docs.rc.fas.harvard.edu/kb/mounting-storage/">RC
documentation is
here</a>]. Figure
out where your home dir is (<code>cd; pwd</code>). It's something like
<code>/n/homeXX/&lt;username&gt;</code>; mine is <code>/n/home14/seddy</code>. The URL to samba
mount my home dir is
<code>smb://rcstore.rc.fas.harvard.edu/homes/home14/seddy</code>. Replace those
last two bits with your own <code>homeXX/&lt;username&gt;</code>.</p>
<p>I have these aliased in my <code>.bashrc</code>:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">alias</span><span class="w"> </span><span class="n">ody</span><span class="o">-</span><span class="n">mount</span><span class="o">=</span><span class="s2">&quot;osascript -e &#39;mount volume </span><span class="se">\&quot;</span><span class="s2">smb://eddyfs.rc.fas.harvard.edu/eddy_lab</span><span class="se">\&quot;</span><span class="s2">&#39;&quot;</span>
<span class="w">    </span><span class="n">alias</span><span class="w"> </span><span class="n">ody</span><span class="o">-</span><span class="n">home</span><span class="o">-</span><span class="n">mount</span><span class="o">=</span><span class="s2">&quot;osascript -e &#39;mount volume </span><span class="se">\&quot;</span><span class="s2">smb://rcstore.rc.fas.harvard.edu/homes/home14/seddy</span><span class="se">\&quot;</span><span class="s2">&#39;&quot;</span>
<span class="w">    </span><span class="n">alias</span><span class="w"> </span><span class="n">ody</span><span class="o">-</span><span class="n">umount</span><span class="o">=</span><span class="s1">&#39;umount /Volumes/eddy_lab&#39;</span>
<span class="w">    </span><span class="n">alias</span><span class="w"> </span><span class="n">ody</span><span class="o">-</span><span class="n">home</span><span class="o">-</span><span class="n">umount</span><span class="o">=</span><span class="s1">&#39;umount /Volumes/seddy&#39;</span>
</code></pre></div>

<p>All reputable people say it's important to remember to unmount the
filesystem before you do something that breaks the network connection
(like logging out of the VPN). On the other hand, I routinely forget,
and nothing has imploded yet.</p>
<hr>
<h2>Accessing our shared data (genomes, seq db's)</h2>
<p>Many standard sequence databases are installed in
<code>/n/eddy_lab/data/</code> including
Pfam, Rfam, and UniProt.</p>
<p>Many genome and transcriptome datasets are installed in
<code>/n/eddy_lab/genomes/</code>.</p>
<hr>
<h2>Working on the cluster</h2>
<p>RC has a zillion software packages installed and available, but most
are not in your <code>${PATH}</code> by default. You load an available package
with the <code>module</code> command. For example, <code>module load hmmer</code> loads RC's
current installed version of HMMER, and <code>module load blast</code> loads
BLAST. <code>module avail</code> lets you behold (almost) everything available,
though it takes a while to run.</p>
<p>You generally work on RC using the SLURM batch scheduler either to
obtain interactive command-line access to a compute node, or to
schedule jobs to run on compute nodes. You shouldn't do any
substantial computation on login nodes. The <code>sbatch</code> command submits
batch scripts to SLURM for later execution.  The <code>srun</code> command runs a
single command on our compute resources interactively.  The <code>sbatch
--wrap="&lt;cmd&gt;"</code> option submits a single command into the batch queue,
and is the most common way that I send jobs to the cluster.</p>
<p>The notes below give useful <code>module</code> and SLURM incantations without a
ton of explication. More thorough RC documentation to skim includes:</p>
<ul>
<li><a href="https://docs.rc.fas.harvard.edu/kb/modules-intro/">Using modules</a></li>
<li><a href="https://docs.rc.fas.harvard.edu/kb/quickstart-guide/">Cluster Quick Start Guide</a></li>
<li><a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/">Running Jobs (with SLURM)</a></li>
</ul>
<h3>the module command; compiling software</h3>
<p>It's ok to compile on a login node (but that's about it). My usual
pre-incantation before working on development of our software (HMMER,
Infernal, Easel):</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="nb">load</span><span class="w"> </span><span class="n">gcc</span><span class="w"> </span><span class="n">git</span><span class="w"> </span><span class="n">valgrind</span><span class="w"> </span><span class="n">python</span>
</code></pre></div>

<p>This loads the <code>gcc</code> compiler, an up-to-date version of <code>git</code>, the
<code>valgrind</code> memory debugging tools, and Python3 (the default python on
RC is Python2).</p>
<p>Other <code>module</code> command examples:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="nb">load</span><span class="w"> </span><span class="n">intel</span><span class="w">        </span><span class="c1"># Intel icc compiler instead</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="nb">load</span><span class="w"> </span><span class="n">gcc</span><span class="w"> </span><span class="n">openmpi</span><span class="w">  </span><span class="c1"># for MPI parallel software, usually I use gcc/OpenMPI</span>

<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">avail</span><span class="w">             </span><span class="c1"># show list of immediately available modules</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">avail</span><span class="w"> </span><span class="n">openmpi</span><span class="w">     </span><span class="c1"># show list of available openmpi modules (there may be different versions installed)</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">list</span><span class="w">              </span><span class="c1"># list my currently loaded modules</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">unload</span><span class="w"> </span><span class="n">gcc</span><span class="w">        </span><span class="c1"># unload a module</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">swap</span><span class="w"> </span><span class="n">intel</span><span class="w"> </span><span class="n">gcc</span><span class="w">    </span><span class="c1"># swap one module out (intel icc) for another (gcc)</span>
<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="n">purge</span><span class="w">             </span><span class="c1"># unload all modules</span>

<span class="w">    </span><span class="n">module</span><span class="w"> </span><span class="nb">load</span><span class="w"> </span><span class="n">gcc</span><span class="o">/</span><span class="mf">7.1</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">fasrc01</span><span class="w"> </span><span class="n">binutils</span><span class="o">/</span><span class="mf">2.29</span><span class="o">-</span><span class="n">fasrc01</span><span class="w">   </span><span class="c1"># sometimes we need to be very specific about versions</span>
</code></pre></div>

<h3>get an interactive cpu node</h3>
<p>Depends on whether you just need a single cpu core (most common),
several cores (for multithreaded software like BLAST or HMMER), or an
entire compute node (40 cores, for the nodes in our <code>-p eddy</code>
partition). My standard incantations are:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nt">srun</span><span class="w"> </span><span class="nt">-p</span><span class="w"> </span><span class="nt">eddy</span><span class="w"> </span><span class="nt">--pty</span><span class="w"> </span><span class="nt">-t</span><span class="w"> </span><span class="nt">6-00</span><span class="p">:</span><span class="nd">00</span><span class="w"> </span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">bash</span><span class="w">                      </span><span class="err">#</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">core</span>
<span class="w">    </span><span class="nt">srun</span><span class="w"> </span><span class="nt">-p</span><span class="w"> </span><span class="nt">eddy</span><span class="w"> </span><span class="nt">--pty</span><span class="w"> </span><span class="nt">-t</span><span class="w"> </span><span class="nt">6-00</span><span class="p">:</span><span class="nd">00</span><span class="w"> </span><span class="nt">-c</span><span class="w"> </span><span class="nt">8</span><span class="w"> </span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">bash</span><span class="w">                 </span><span class="err">#</span><span class="w"> </span><span class="nt">multiple</span><span class="w"> </span><span class="nt">cores</span><span class="o">;</span><span class="w"> </span><span class="nt">here</span><span class="w"> </span><span class="nt">8</span>
<span class="w">    </span><span class="nt">srun</span><span class="w"> </span><span class="nt">-p</span><span class="w"> </span><span class="nt">eddy</span><span class="w"> </span><span class="nt">--pty</span><span class="w"> </span><span class="nt">-t</span><span class="w"> </span><span class="nt">6-00</span><span class="p">:</span><span class="nd">00</span><span class="w"> </span><span class="nt">-c</span><span class="w"> </span><span class="nt">40</span><span class="w"> </span><span class="nt">--exclusive</span><span class="w"> </span><span class="o">/</span><span class="nt">bin</span><span class="o">/</span><span class="nt">bash</span><span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nt">entire</span><span class="w"> </span><span class="nt">node</span>
</code></pre></div>

<ul>
<li><code>-p eddy</code> says you want one of the nodes in our main cpu partition. </li>
<li><code>--pty</code> says you want an interactive terminal.</li>
<li><code>-t 6-00:00</code> says you want it assigned to you for up to 6
  days. You're not going to use it that long - you're going to log out
  when you're done (right?), but RC needs you to specify an estimated
  time.</li>
<li><code>-c 8</code> or <code>-c 40</code> says you want to use 8 or 40 cores (or whatever). All the machines in
  the <code>-p eddy</code> partition have 40 cores. I think if you don't specify
  this, even if you get exclusive access to the node, you may only be
  able to use 1 core on it.</li>
<li><code>--exclusive</code> says that no other job will be allowed to start on
  your node while you're using it.</li>
</ul>
<h3>run one command</h3>
<p>The most common way I submit tasks to the cluster is one command at a
time.  The <code>--wrap</code> option to <code>sbatch</code> lets you submit a job without
having to write a SLURM script for it.</p>
<div class="highlight"><pre><span></span><code>    sbatch -t 6-00:00 -p eddy -c 4 -N 1 --wrap=&quot;hmmsearch --cpu 4 fn3.hmm /n/eddyfs01/data/dbs/Uniprot/uniprot_sprot.fasta&quot;
</code></pre></div>

<p><code>hmmsearch</code> is multithreaded; I'm matching the number of cores I
request from SLURM (<code>-c 4</code>) to the number of cores I'm telling
hmmsearch to use (<code>--cpu 4</code>), and telling SLURM I want them all on one
compute node (<code>-N 1</code>). For a non-parallel program, you'd leave off the
<code>-c 4</code> in the <code>sbatch</code> options.</p>
<p>The stdout goes to a SLURM outfile, something like
<code>slurm-56384497.out</code>. Or you can add a shell redirect <code>&gt; foo.out</code> to
your <code>--wrap</code> command if you like.</p>
<h3>running a few commands, looping over some input files</h3>
<p>A couple of examples of ways to submit several commands at once:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span>ls<span class="w"> </span>*.gz<span class="w"> </span>|<span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span>{}<span class="w">  </span>sbatch<span class="w"> </span>--wrap=&quot;gunzip<span class="w"> </span>-c<span class="w"> </span>{}&quot;<span class="w">   </span>#<span class="w"> </span>uncompress<span class="w"> </span>all<span class="w"> </span>.gz<span class="w"> </span>files<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>directory

<span class="w">    </span>for<span class="w"> </span>FILE<span class="w"> </span>in<span class="w"> </span>*.fa;<span class="w"> </span>do
<span class="w">      </span>echo<span class="w"> </span><span class="cp">${</span><span class="n">FILE</span><span class="cp">}</span>
<span class="w">      </span>sbatch<span class="w"> </span>-p<span class="w"> </span>eddy<span class="w"> </span>-t<span class="w"> </span>10<span class="w"> </span>--wrap=&quot;gzip<span class="w"> </span><span class="cp">${</span><span class="n">FILE</span><span class="cp">}</span>&quot;<span class="w">          </span>#<span class="w"> </span>compress<span class="w"> </span>all<span class="w"> </span>.fa<span class="w"> </span>files<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>directory
<span class="w">      </span>sleep<span class="w"> </span>1<span class="w">                                             </span>#<span class="w"> </span>pausing<span class="w"> </span>between<span class="w"> </span>submissions<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>kind<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>scheduler
<span class="w">    </span>done
</code></pre></div>

<h3>writing an sbatch script</h3>
<p>If your job is more complicated than a single command - for example,
if it depends on loading software with a <code>module load</code> command first -
you can write an <code>sbatch</code> script. The SLURM options go into the
script, instead of on the <code>sbatch</code> command line, using a special
format. An example that (stupidly) loads gcc and just calls
<code>hostname</code>, so the output will be the name of the compute node the
script ran on:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -c 1        # Number of cores/threads</span>
<span class="c1">#SBATCH -N 1        # Ensure that all cores are on one machine</span>
<span class="c1">#SBATCH -t 6-00:00  # Runtime in D-HH:MM</span>
<span class="c1">#SBATCH -p eddy     # Partition to submit to</span>
<span class="c1">#SBATCH --mem=4000  # Memory pool for all cores (see also --mem-per-cpu)</span>
<span class="c1">#SBATCH -o myjob_%j.out     # File to which STDOUT will be written; %j is the job number assigned by SLURM</span>
<span class="c1">#SBATCH -e myjob_%j.err     # File to which STDERR will be written</span>

module<span class="w"> </span>load<span class="w"> </span>gcc
hostname
</code></pre></div>

<p>Save this to a file (<code>foo.sh</code> for example) and submit it with <code>sbatch</code>:</p>
<div class="highlight"><pre><span></span><code>    sbatch foo.sh
</code></pre></div>

<h3>running lots of commands</h3>
<p>If you have to submit lots of jobs (hundreds or thousands) to the
cluster at once, the preferred way to do it is with <strong>job arrays</strong>, to
avoid overloading the scheduler. See the
<a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/#Job_arrays">RC documentation</a>.</p>
<h3>monitoring your running jobs</h3>
<p>The <code>sinfo</code> command shows information about what's currently running where.</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">sinfo</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="n">eddy</span><span class="w">    </span><span class="c1"># show the state of the `eddy` partition</span>
<span class="w">    </span><span class="n">sinfo</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w">         </span><span class="c1"># show the state of all RC partitions</span>
</code></pre></div>

<p>The <code>sacct</code> command shows SLURM log info about jobs you ran in the past.</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">sacct</span><span class="w">                                                      </span><span class="c1"># default log</span>
<span class="w">    </span><span class="n">sacct</span><span class="w"> </span><span class="o">--</span><span class="k">format</span><span class="o">=</span><span class="n">jobid</span><span class="p">,</span><span class="n">jobname</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">cputime</span><span class="p">,</span><span class="n">elapsed</span><span class="p">,</span><span class="n">maxrss</span><span class="w">  </span><span class="c1"># custom-formatted to get cputime (`cputime`), max mem (`maxrss`) used</span>
<span class="w">    </span><span class="n">sacct</span><span class="w"> </span><span class="o">--</span><span class="k">name</span><span class="o">=</span><span class="n">B1n</span><span class="o">-</span><span class="mf">2.</span><span class="n">sh</span><span class="w">                                      </span><span class="c1"># --name=&lt;jobname&gt; to see a particular job</span>
<span class="w">    </span><span class="n">sacct</span><span class="w"> </span><span class="o">-</span><span class="n">S</span><span class="w"> </span><span class="mi">2020</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">29</span><span class="w">                                        </span><span class="c1"># see jobs you started on 3/29/2020</span>
</code></pre></div>

<h3>etiquette</h3>
<p>To avoid getting in each others' way on our <code>-p eddy</code> partition, at any one time,
please limit your resource use to:</p>
<ul>
<li>&lt;50% of our cores</li>
<li>&lt;50% of the RAM per node OR &lt;4GB/core</li>
<li>&lt;30min per job</li>
</ul>
<p>These guidelines are just guidelines.  The principle is what's
important.  Nobody should have to wait &gt;30min to get their job to
start running on <code>-p eddy</code>. If you cause such pain, there may be
public shaming and/or donut penalty.  Launching one job that takes a
day to complete, or a thousand 10-second jobs is not going to get in
anyone's way either. Conversely, it's possible that two or more people
in the lab could try to occupy 50% of our resources at a time and jam
us up, so use <code>sinfo -p eddy</code> to see how busy things are and be
reasonable.</p>
<p>You can also add <code>--nice 1000</code> to your <code>sbatch</code> command, to downgrade
your running priority in the queue, which helps let other people's
jobs get run before yours.</p>

        </article>
        <footer>Powered by <a href="https://getpelican.com/">Pelican</a>.  Site theme is a modified version of <a href="https://github.com/andrewheiss/ath-tufte-pelican">ath-tufte-pelican</a>.</footer>
    </div>
    </body>
</html>